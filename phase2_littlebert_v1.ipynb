{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled108.ipynb",
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "322725595537433bb35c2d6c608c1a0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_047a94db5dff498da83846e8bcae1daa",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_830344ba04704bbd8c5072bb79227664",
              "IPY_MODEL_c5b9faa4afa64224b3b41f06868c47df"
            ]
          }
        },
        "047a94db5dff498da83846e8bcae1daa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "830344ba04704bbd8c5072bb79227664": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_2870c441065e42edae952be168747fa8",
            "_dom_classes": [],
            "description": " 14%",
            "_model_name": "FloatProgressModel",
            "bar_style": "danger",
            "max": 9642,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1362,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1cb1f5d208a74639b9a5afb8e9711c23"
          }
        },
        "c5b9faa4afa64224b3b41f06868c47df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_a6227d7b130d49a987c6d5157eb4fa08",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1362/9642 [07:18&lt;48:36,  2.84it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_953c4103e46b4dc080539ec10e3310b7"
          }
        },
        "2870c441065e42edae952be168747fa8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1cb1f5d208a74639b9a5afb8e9711c23": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a6227d7b130d49a987c6d5157eb4fa08": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "953c4103e46b4dc080539ec10e3310b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mmsamiei/thesis-prototype/blob/master/phase2_littlebert_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ep8U0taIvluE",
        "colab_type": "code",
        "outputId": "e4f5c340-9c8a-4e9d-8867-e3965ae61336",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4s1AOzexA6E",
        "colab_type": "code",
        "outputId": "4b74542f-803a-40c0-8f31-debcdba58560",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 622
        }
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/10/aeefced99c8a59d828a92cc11d213e2743212d3641c87c82d61b035a7d5c/transformers-2.3.0-py3-none-any.whl (447kB)\n",
            "\r\u001b[K     |▊                               | 10kB 29.7MB/s eta 0:00:01\r\u001b[K     |█▌                              | 20kB 1.7MB/s eta 0:00:01\r\u001b[K     |██▏                             | 30kB 2.5MB/s eta 0:00:01\r\u001b[K     |███                             | 40kB 1.7MB/s eta 0:00:01\r\u001b[K     |███▋                            | 51kB 2.1MB/s eta 0:00:01\r\u001b[K     |████▍                           | 61kB 2.5MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 71kB 2.9MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 81kB 3.3MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 92kB 3.7MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 102kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████                        | 112kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 122kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 133kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 143kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████                     | 153kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 163kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 174kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 184kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 194kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 204kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 215kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 225kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 235kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 245kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 256kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 266kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 276kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 286kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 296kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 307kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 317kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 327kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 337kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 348kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 358kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 368kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 378kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 389kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 399kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 409kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 419kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 430kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 440kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 450kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.9)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.10.40)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 64.1MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/b4/7a41d630547a4afd58143597d5a49e07bfd4c42914d8335b2a5657efc14b/sacremoses-0.0.38.tar.gz (860kB)\n",
            "\u001b[K     |████████████████████████████████| 870kB 66.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.2.1)\n",
            "Requirement already satisfied: botocore<1.14.0,>=1.13.40 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.13.40)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: python-dateutil<2.8.1,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.40->boto3->transformers) (2.6.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.40->boto3->transformers) (0.15.2)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.38-cp36-none-any.whl size=884629 sha256=a374841aee78fcd123ec06e0f133c12aa59fc521c9014d2dc19ce80370cd89d5\n",
            "  Stored in directory: /root/.cache/pip/wheels/6d/ec/1a/21b8912e35e02741306f35f66c785f3afe94de754a0eaf1422\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sentencepiece, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.38 sentencepiece-0.1.85 transformers-2.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kK1Sy27mxGwr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp /content/drive/My\\ Drive/Thesis/phase-2/tokenized_albert-base_v1_phase2.json ./train.json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gOE8Bhu5xo9S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "import os\n",
        "import torch\n",
        "import json\n",
        "from torch.utils.data.sampler import SubsetRandomSampler"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxEtXDgvxqa-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyDataset(Dataset):\n",
        "    \"\"\"My dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, json_file):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            json_file (string): Path to the json file with annotations.\n",
        "        \"\"\"\n",
        "        self.dialogues = json.load(open(json_file))\n",
        "        self.root_dir = json_file\n",
        "\n",
        "        self.dialogues = sorted(self.dialogues, key=lambda x: len(x['history']))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dialogues)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      \n",
        "        \n",
        "        history_lst = self.dialogues[idx]['history']\n",
        "        true_lst = self.dialogues[idx]['true_sentence']\n",
        "        false_lst = self.dialogues[idx]['false_sentenc']\n",
        "\n",
        "\n",
        "        if(len(history_lst)>40):\n",
        "          history_lst = history_lst[-40:]\n",
        "\n",
        "        if(len(true_lst)>40):\n",
        "          true_lst = true_lst[:40]\n",
        "        \n",
        "        if(len(false_lst)>40):\n",
        "          false_lst = false_lst[:40]\n",
        "        \n",
        "\n",
        "        history = torch.LongTensor(history_lst)\n",
        "        true_sample = torch.LongTensor(true_lst)\n",
        "        false_sample = torch.LongTensor(false_lst)\n",
        "\n",
        "        sample = {'history': history, 'true_sample': true_sample, 'false_sample': false_sample}\n",
        "\n",
        "        return sample"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2i9u5-PxvbC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = MyDataset('train.json')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIl0JyXfxv6G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def my_collate_fn(batch):\n",
        "\n",
        "  len_batch = len(batch)\n",
        "\n",
        "  \n",
        "  max_len_history = max([len(data['history']) for data in batch])\n",
        "  max_len_true_sample = max([len(data['true_sample']) for data in batch])\n",
        "  max_len_false_sample = max([len(data['false_sample']) for data in batch])\n",
        "  \n",
        "  padding_ind = 1 ## for bert is 0 but because embedding hate 0!! we used 1\n",
        "  result_history = torch.ones(len_batch, max_len_history)\n",
        "  result_true_sample = torch.ones(len_batch, max_len_true_sample)\n",
        "  result_false_sample = torch.ones(len_batch, max_len_false_sample)\n",
        "\n",
        "  for i, data in enumerate(batch):\n",
        "    p1 = len(data['history'])\n",
        "    result_history[i, :p1] = data['history']\n",
        "    p2 = len(data['true_sample'])\n",
        "    result_true_sample[i, :p2] = data['true_sample']\n",
        "    p3 = len(data['false_sample'])\n",
        "    result_false_sample[i, :p3] = data['false_sample']\n",
        "\n",
        "\n",
        "\n",
        "  return result_history.long(), result_true_sample.long(), result_false_sample.long()\n",
        "\n",
        "sampler = torch.utils.data.SequentialSampler(dataset)\n",
        "dataset_loader = torch.utils.data.DataLoader(dataset, batch_size=256, sampler=sampler,\n",
        "                                             shuffle=False, collate_fn=my_collate_fn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJRtnohSxzMh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for _, batch in enumerate(dataset_loader):\n",
        "  history, true_sample, false_sample = batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_orN_TyyGjX",
        "colab_type": "text"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eW4SDG5iyI7n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olztAZtTyK2n",
        "colab_type": "code",
        "outputId": "86431900-93c1-43f9-8774-89725136304c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        }
      },
      "source": [
        "import torch\n",
        "from transformers import *\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "\n",
        "class RepresentModel(nn.Module):\n",
        "  \n",
        "  def __init__(self, device):\n",
        "    super().__init__()\n",
        "\n",
        "    self.device = device\n",
        "    \n",
        "    self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "    del self.bert.encoder.layer[3:]\n",
        "\n",
        "    for p in self.bert.encoder.layer[0:2].parameters():\n",
        "      p.requires_grad = False\n",
        "\n",
        "    for p in self.bert.embeddings.parameters():\n",
        "      p.requires_grad = False\n",
        "\n",
        "\n",
        "    self.fc = nn.Linear(768, 128)\n",
        "\n",
        "  def forward(self, x):\n",
        "    ## x = [batch, sent]\n",
        "    batch_size, sent_len = x.shape[0], x.shape[1]\n",
        "\n",
        "    temp = self.bert(x)\n",
        "    temp = temp[0]\n",
        "    ## [batch, sent_len, 768]\n",
        "    temp = temp[:,0,:]\n",
        "    ## [batch, 768]\n",
        "    temp = self.fc(temp)\n",
        "    return temp\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9VljCsZvy_Vo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hid_size = 768\n",
        "model = RepresentModel(device).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZ5OxkLVzFj7",
        "colab_type": "code",
        "outputId": "eb904c0c-320a-4c65-9d98-974a0752370f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "test_len = 20\n",
        "batch_size = 64\n",
        "vocab_size = 30000\n",
        "test_input = torch.LongTensor(batch_size, test_len).random_(1,vocab_size).to(device)\n",
        "model(test_input).shape"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([64, 128])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWnSQwT0zm_w",
        "colab_type": "code",
        "outputId": "f48fbe24-7142-4687-f610-8bcbefa124c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 7,776,896 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tqBrmPCT5lUo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NoamOpt:\n",
        "    \"Optim wrapper that implements rate.\"\n",
        "    def __init__(self, model_size, factor, warmup, optimizer):\n",
        "        self.optimizer = optimizer\n",
        "        self._step = 0\n",
        "        self.warmup = warmup\n",
        "        self.factor = factor\n",
        "        self.model_size = model_size\n",
        "        self._rate = 0\n",
        "        \n",
        "    def step(self):\n",
        "        \"Update parameters and rate\"\n",
        "        self._step += 1\n",
        "        rate = self.rate()\n",
        "        for p in self.optimizer.param_groups:\n",
        "            p['lr'] = rate\n",
        "        self._rate = rate\n",
        "        self.optimizer.step()\n",
        "        \n",
        "    def rate(self, step = None):\n",
        "        \"Implement `lrate` above\"\n",
        "        if step is None:\n",
        "            step = self._step\n",
        "        return self.factor * \\\n",
        "            (self.model_size ** (-0.5) *\n",
        "            min(step ** (-0.5), step * self.warmup ** (-1.5)))\n",
        "    \n",
        "    def zero_grad(self):\n",
        "        self.optimizer.zero_grad()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNzlUyIz5nN_",
        "colab_type": "code",
        "outputId": "423590f5-2792-46d6-b9fe-6df0b8610c69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "!pip install -U tqdm"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tqdm\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/72/c9/7fc20feac72e79032a7c8138fd0d395dc6d8812b5b9edf53c3afd0b31017/tqdm-4.41.1-py2.py3-none-any.whl (56kB)\n",
            "\r\u001b[K     |█████▊                          | 10kB 25.8MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 20kB 1.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 30kB 2.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 40kB 1.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 51kB 2.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61kB 2.1MB/s \n",
            "\u001b[?25hInstalling collected packages: tqdm\n",
            "  Found existing installation: tqdm 4.28.1\n",
            "    Uninstalling tqdm-4.28.1:\n",
            "      Successfully uninstalled tqdm-4.28.1\n",
            "Successfully installed tqdm-4.41.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tqdm"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VmcJkmOB5qTj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tqdm.notebook import tqdm\n",
        "\n",
        "def train_one_epoch(model,train_iter, optimizer, criterion, clip):\n",
        "  epoch_loss = 0\n",
        "  model.train()\n",
        "  for batch in tqdm(train_iter):\n",
        "    optimizer.zero_grad()\n",
        "    history, true_sample, false_sample = batch\n",
        "    batch_size = history.shape[0]\n",
        "    ### hist = [batch, sent_len]\n",
        "    history_rpr = model(history.to(device))\n",
        "    true_rpr = model(true_sample.to(device))\n",
        "    false_rpr = model(false_sample.to(device))\n",
        "    ## rpr = [batch_size, hidden]\n",
        "    cos = nn.PairwiseDistance().to(device)\n",
        "    tru_sml = -1*cos(history_rpr, true_rpr)\n",
        "    fls_sml = -1*cos(history_rpr, false_rpr)\n",
        "    mini_batch_tensor = torch.ones(batch_size).to(device)\n",
        "    loss = criterion(tru_sml, fls_sml, mini_batch_tensor)\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "    optimizer.step()\n",
        "    epoch_loss += loss.item()\n",
        "  return epoch_loss / len(train_iter)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RWpJPNSi5soj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, train_iter, optimizer, criterion, clip, N_EPOCH):\n",
        "  for epoch in range(N_EPOCH):\n",
        "    epoch_loss = train_one_epoch(model, train_iter, optimizer, criterion, clip)\n",
        "    print(epoch_loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4xKYx8t5uf7",
        "colab_type": "code",
        "outputId": "41dc19db-d2dc-498e-8527-5e3754ebd7d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386,
          "referenced_widgets": [
            "322725595537433bb35c2d6c608c1a0b",
            "047a94db5dff498da83846e8bcae1daa",
            "830344ba04704bbd8c5072bb79227664",
            "c5b9faa4afa64224b3b41f06868c47df",
            "2870c441065e42edae952be168747fa8",
            "1cb1f5d208a74639b9a5afb8e9711c23",
            "a6227d7b130d49a987c6d5157eb4fa08",
            "953c4103e46b4dc080539ec10e3310b7"
          ]
        }
      },
      "source": [
        "optimizer = NoamOpt(hid_size, 1, 2000,\n",
        "              torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))\n",
        "criterion = torch.nn.MarginRankingLoss(margin=10).to(device)\n",
        "train(model, dataset_loader, optimizer, criterion, 1, 1)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "322725595537433bb35c2d6c608c1a0b",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=9642.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-4c3c8c9f391a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m               torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))\n\u001b[1;32m      3\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMarginRankingLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmargin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-17-14563e5d9bd4>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_iter, optimizer, criterion, clip, N_EPOCH)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_EPOCH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN_EPOCH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mepoch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-e82b4186fa44>\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, train_iter, optimizer, criterion, clip)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mtru_sml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory_rpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_rpr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mfls_sml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory_rpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfalse_rpr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mmini_batch_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtru_sml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfls_sml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmini_batch_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}